{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1kUSPRMF34S92UPPiAN1Sa7taf7uNnACl",
      "authorship_tag": "ABX9TyNFjgbdykG8A7SOwm3uKpvm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SergioCarmo-ro/monit_aluno_tempo_real/blob/main/monit_aluno_tempo_real.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PROJETO MONITORAMENTO DE ALUNOS EM TEMPO REAL**"
      ],
      "metadata": {
        "id": "HpyTdRIlaGwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install tensorflow\n",
        "!pip install pyrealsense2\n",
        "!pip install mtcnn\n",
        "!pip install opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ojVZzJZRmPr-",
        "outputId": "92927067-d36d-4e5b-fc63-8d277a077251"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: pyrealsense2 in /usr/local/lib/python3.12/dist-packages (2.56.5.9235)\n",
            "Requirement already satisfied: mtcnn in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from mtcnn) (1.5.2)\n",
            "Requirement already satisfied: lz4>=4.3.3 in /usr/local/lib/python3.12/dist-packages (from mtcnn) (4.4.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BosK0h03Zgfl"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import cv2\n",
        "import numpy as np\n",
        "from scipy.io import loadmat, savemat\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import mtcnn\n",
        "from mtcnn.mtcnn import MTCNN\n",
        "from matplotlib.patches import Rectangle\n",
        "from os import listdir\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "# Importe funções do repo (ex.: utils, renderer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "400ee585",
        "outputId": "fa9b1615-07ad-4b07-ea24-d89f0a5722b0"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Create an empty graph definition\n",
        "graph_def = tf.compat.v1.GraphDef()\n",
        "\n",
        "# Specify the path to your protobuf file\n",
        "protobuf_file_path = 'path/to/your/model.pb'\n",
        "\n",
        "# Open the protobuf file and parse its content into the graph definition\n",
        "try:\n",
        "    with tf.io.gfile.GFile(protobuf_file_path, 'rb') as f:\n",
        "        graph_def.ParseFromString(f.read())\n",
        "\n",
        "    # Import the graph definition into the default TensorFlow graph\n",
        "    tf.import_graph_def(graph_def, name='')\n",
        "\n",
        "    print(f\"Successfully loaded graph definition from '{protobuf_file_path}'\")\n",
        "\n",
        "except tf.errors.NotFoundError:\n",
        "    print(f\"Error: The file '{protobuf_file_path}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: The file 'path/to/your/model.pb' was not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0360fcf",
        "outputId": "ae1e48d0-57ae-4036-a962-e84fb1dd8d5d",
        "collapsed": true
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify the folder path in Google Drive\n",
        "folder_path = '/content/drive/My Drive/'\n",
        "\n",
        "# List files in the folder\n",
        "try:\n",
        "    for fn in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, fn)\n",
        "        if os.path.isfile(file_path):\n",
        "            file_size = os.path.getsize(file_path)\n",
        "            print('Found file \"{name}\" with length {length} bytes'.format(\n",
        "                name=fn, length=file_size))\n",
        "        elif os.path.isdir(file_path):\n",
        "            print('Found directory \"{name}\"'.format(name=fn))\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The folder '{folder_path}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found directory \"Colab Notebooks\"\n",
            "Found directory \"pthread (1)\"\n",
            "Found directory \"pthread\"\n",
            "Found directory \"Classroom\"\n",
            "Found file \"CM_SérgioAdemirRochadoCarmo.pdf\" with length 27319 bytes\n",
            "Found file \"He_SérgioAdemirRochado Carmo.pdf\" with length 11314 bytes\n",
            "Found file \"CAPA TRAB. EVENTUAL 1.pdf\" with length 157891 bytes\n",
            "Found file \"Atividade Eventual 1 - FTC.pdf\" with length 176190 bytes\n",
            "Found directory \"Atividade p N2\"\n",
            "Found directory \"Ativ. P  Prova N3\"\n",
            "Found file \"Trabalho 2.pdf\" with length 513550 bytes\n",
            "Found directory \"Documentos meus \"\n",
            "Found file \"report (9).pdf\" with length 26929 bytes\n",
            "Found file \"Certificado.pdf\" with length 201687 bytes\n",
            "Found file \"Certificado Curva ABC.pdf\" with length 201170 bytes\n",
            "Found file \"Certificado Introdução a Linguagem Kotlen .pdf\" with length 200410 bytes\n",
            "Found file \"algoritmo Bio Inspirados.pdf\" with length 200756 bytes\n",
            "Found file \"Logística .pdf\" with length 200009 bytes\n",
            "Found file \"Marchin Learne.pdf\" with length 200894 bytes\n",
            "Found directory \"POO\"\n",
            "Found directory \"Windows 11, 10, 8.1\"\n",
            "Found file \"Historico Analitico.pdf\" with length 10272 bytes\n",
            "Found file \"HE_SérgioCarmo.pdf\" with length 10272 bytes\n",
            "Found file \"RG CPF (1).pdf\" with length 1117783 bytes\n",
            "Found file \"RG CPF.pdf\" with length 1117783 bytes\n",
            "Found file \"CM_SérgioCarmo.pdf\" with length 26928 bytes\n",
            "Found file \"POO-TP1-Matheus.José.Mauricio.Sérgio.Victor.gdoc\" with length 182 bytes\n",
            "Found directory \"pcr-money-main\"\n",
            "Found file \"TRAB_BD - TRASPORTE CARGA AÉREA (2).docx\" with length 68217 bytes\n",
            "Found file \"BeerNotSys.pptx\" with length 5971057 bytes\n",
            "Found file \"C Avançado - Aula 7.1_ Resolução de trabalhos.mp4\" with length 135573659 bytes\n",
            "Found file \"C Avançado - Aula 5.1_ Pesquisa sequencial e pesquisa binária.mp4\" with length 66538471 bytes\n",
            "Found file \"DocumentosPessoais_SérgioCarmo.pdf\" with length 601327 bytes\n",
            "Found file \"AutorizaçãoUsoImagem_.docx\" with length 619052 bytes\n",
            "Found file \"TabelaTermoCompromisso_.docx\" with length 14397 bytes\n",
            "Found file \"HE_SérgioDoCarmo (1).pdf\" with length 10270 bytes\n",
            "Found file \"HE_SérgioDoCarmo.pdf\" with length 10270 bytes\n",
            "Found file \"CM_SérgioDoCarmo.pdf\" with length 26929 bytes\n",
            "Found file \"Conversa do WhatsApp com Adonias.txt\" with length 2427 bytes\n",
            "Found file \"VID-20230306-WA0016.mp4\" with length 4945180 bytes\n",
            "Found file \"PTT-20230306-WA0023.opus\" with length 13696 bytes\n",
            "Found file \"VID-20230306-WA0024.mp4\" with length 7712291 bytes\n",
            "Found file \"PTT-20230307-WA0001.opus\" with length 23771 bytes\n",
            "Found file \"PTT-20230307-WA0042.opus\" with length 10458 bytes\n",
            "Found file \"AUD-20230308-WA0016.opus\" with length 40619 bytes\n",
            "Found file \"AUD-20230308-WA0017.opus\" with length 32380 bytes\n",
            "Found file \"AUD-20230308-WA0018.opus\" with length 34886 bytes\n",
            "Found file \"AUD-20230308-WA0019.opus\" with length 54672 bytes\n",
            "Found file \"PTT-20230308-WA0021.opus\" with length 38913 bytes\n",
            "Found file \"AUD-20230308-WA0020.opus\" with length 20766 bytes\n",
            "Found directory \"Super Ufam \"\n",
            "Found file \"Pro. Dr. Jean MG.mp4\" with length 36333265 bytes\n",
            "Found file \"github.jpg\" with length 4269487 bytes\n",
            "Found file \"Rede de Baixa latência.mp4\" with length 31874788 bytes\n",
            "Found file \"Evelin Lições Aprendidas.mp4\" with length 78188696 bytes\n",
            "Found file \"Super Teck Week Diversão no Hall.mp4\" with length 77371667 bytes\n",
            "Found file \"TRABALHO ARQUITETURA DE SOFTWARE.gdoc\" with length 182 bytes\n",
            "Found file \"exercicio Eng.II.pdf\" with length 149169 bytes\n",
            "Found file \"Modelo_Projeto_Engenharia de Software - ECollect.gdoc\" with length 182 bytes\n",
            "Found file \"Livro de Sistemas de Banco de Dados 7ª.pdf\" with length 557864067 bytes\n",
            "Found file \"COMO QUEBRAR O CODIGO.pdf\" with length 97443302 bytes\n",
            "Found file \"ATIVIDADE ECONÔMICA.gdoc\" with length 182 bytes\n",
            "Found file \"Anexo_1478508.gdoc\" with length 182 bytes\n",
            "Found file \"Documento de Arquitetura de Software do SPEU 1-Exemplo-RUP (1).gdoc\" with length 182 bytes\n",
            "Found file \"Documento-de-arquitetura-de-software.gdoc\" with length 182 bytes\n",
            "Found file \"AULA 3 e 4 (1).gdoc\" with length 182 bytes\n",
            "Found directory \"test (1)\"\n",
            "Found directory \"test\"\n",
            "Found file \"Documento de Arquitetura de Software do SPEU 1-Exemplo-RUP.gdoc\" with length 182 bytes\n",
            "Found file \"Documento Arquitetura DONAR.gdoc\" with length 182 bytes\n",
            "Found file \"command.gdoc\" with length 182 bytes\n",
            "Found file \"Trabalho AEDII.gdoc\" with length 182 bytes\n",
            "Found file \"Apresentação Trabalho1.gslides\" with length 182 bytes\n",
            "Found file \"Trabalho AEDII.docx\" with length 400168 bytes\n",
            "Found file \"TRABALHO PRÁTICO 01 DA DISCIPLINA DE ALGORITMOS\u000b E ESTRUTURA DE DADOS II.gslides\" with length 182 bytes\n",
            "Found file \"Cronograma_de_grupos_de_estudo.gdoc\" with length 182 bytes\n",
            "Found file \"Referencias_II_093205.gdoc\" with length 182 bytes\n",
            "Found directory \"AEDII Árvore Paciente \"\n",
            "Found file \"Trabalho de BANCO DE DADOS.gdoc\" with length 182 bytes\n",
            "Found file \"TRAB_BD - TRASPORTE CARGA AÉREA (1).docx\" with length 212594 bytes\n",
            "Found file \"Trabalho_2 -  AEDII.docx\" with length 299762 bytes\n",
            "Found file \"exercicio Eng.II.gdoc\" with length 182 bytes\n",
            "Found file \"RA_ICET_AluWP2_ICTJr_SergiodoCarmo_07.2023_091119.gdoc\" with length 182 bytes\n",
            "Found file \"RA_ICET_AluWP2_ICTJr_SergiodoCarmo_07.2023_100312.gdoc\" with length 182 bytes\n",
            "Found file \"RA_ICET_AluWP2_ICTJr_SergiodoCarmo_07.2023_100312.pdf\" with length 354223 bytes\n",
            "Found file \"RA_ICET_AluWP2_ICTJr_SergiodoCarmo_07.2023_100312.docx\" with length 1083988 bytes\n",
            "Found file \"PROJETO SGL - Urb.pdf\" with length 397606 bytes\n",
            "Found file \"Álgebra linear_ uma introdução moderna ( PDFDrive ).pdf\" with length 207656232 bytes\n",
            "Found file \"Álgebra Linear- Steinbruch (1).pdf\" with length 34036570 bytes\n",
            "Found file \"Livro de C-C++ - A Bíblia.pdf\" with length 35789003 bytes\n",
            "Found file \"Álgebra Linear- Steinbruch.pdf\" with length 34036570 bytes\n",
            "Found file \"Livro_de_Android_Dominando_o_Android_do_Básico_ao_Avançado.pdf\" with length 79096642 bytes\n",
            "Found file \"Ensalamento 2023-1 ENGSOFT.pdf\" with length 20204 bytes\n",
            "Found file \"Algebra_Linear_Colecao_Schaum_4a_ed_Seym.pdf\" with length 31573833 bytes\n",
            "Found directory \"SUPER_Relacionados ao Nosso Trabalho\"\n",
            "Found file \"Cópia de Criando um App em Kotlin parte 1.gslides\" with length 182 bytes\n",
            "Found file \"Sérgio_Ademir_Rocha_do_Carmo[1]\" with length 230683 bytes\n",
            "Found file \"Sérgio_Ademir_Rocha_do_Carmo[1].docx\" with length 63248 bytes\n",
            "Found file \"Cópia de Cópia de Criando um App em Kotlin parte 4.gslides\" with length 182 bytes\n",
            "Found file \"TRAB. ESTATISTICA.R\" with length 3487 bytes\n",
            "Found file \"TRABALHO.docx\" with length 15286 bytes\n",
            "Found directory \"jperf-2.0.2 (1)\"\n",
            "Found file \"Aula 07 - Protocolos TCP e UDP.gslides\" with length 182 bytes\n",
            "Found file \"Aula 08 -  QoS.gdoc\" with length 182 bytes\n",
            "Found directory \"jperf-2.0.2\"\n",
            "Found file \"IMG_20231020_185242.jpg\" with length 4486532 bytes\n",
            "Found file \"IMG_20231020_184503.jpg\" with length 5093767 bytes\n",
            "Found file \"sinal sete de setembro x nossa senhora do rosário sentido norte (2).mp4\" with length 2096642 bytes\n",
            "Found directory \"Gestão da Qualidade de Software.\"\n",
            "Found file \"PrimeiroAppKotlin.rar\" with length 39049044 bytes\n",
            "Found file \"Inferencia - Parte 1.R\" with length 2196 bytes\n",
            "Found file \"Sergio_Carmo.pdf\" with length 10781 bytes\n",
            "Found file \"Criando um App em Kotlin parte 1.pptx\" with length 1767833 bytes\n",
            "Found file \"Informações de contato.gform\" with length 182 bytes\n",
            "Found file \"Projeto_Sistema Mobile para a Inclusão Social de Crianças em Dificuldades Motoras (1) C (1)completo.gdoc\" with length 182 bytes\n",
            "Found file \"HE_SergioCarmo (1).pdf\" with length 10822 bytes\n",
            "Found file \"HE_SergioCarmo.pdf\" with length 10822 bytes\n",
            "Found directory \"Material Didatico\"\n",
            "Found file \"HE_SérgiodoCarmo (1).pdf\" with length 10823 bytes\n",
            "Found file \"HE_SérgiodoCarmo.pdf\" with length 10823 bytes\n",
            "Found file \"RG e CPF.pdf\" with length 358606 bytes\n",
            "Found directory \"sladbar-lateral (1)\"\n",
            "Found directory \"sladbar-lateral\"\n",
            "Found file \"Aula 02 - 12.012.2023 (1).gdoc\" with length 182 bytes\n",
            "Found file \"Aula 02 - 12.012.2023.gdoc\" with length 182 bytes\n",
            "Found directory \"sladbar_carrosel\"\n",
            "Found directory \"layout_estruturado\"\n",
            "Found file \"PROJETO PROGRAMAÇÃO MOVEL.gdoc\" with length 182 bytes\n",
            "Found directory \"semaforos.zip (Unzipped Files)\"\n",
            "Found file \"Documentos.pdf\" with length 1113628 bytes\n",
            "Found file \"AutorizaçãoUsoImagem_SergioCarmo.docx\" with length 1157787 bytes\n",
            "Found file \"TabelaTermoCompromisso_SérgioCarmo.docx\" with length 13904 bytes\n",
            "Found file \"Planilha sem título.gsheet\" with length 182 bytes\n",
            "Found file \"Cronograma A jornada SQL.gsheet\" with length 182 bytes\n",
            "Found file \"Plano de gerenciamento do cronograma.gdoc\" with length 182 bytes\n",
            "Found file \"Template_II Seminário do WP2 do Projeto SUPER no ICET - Ano 4 (SWP2-ICET).gslides\" with length 182 bytes\n",
            "Found directory \"projeto.zip (Unzipped Files) (1)\"\n",
            "Found file \"pythonProject1.tar.gz\" with length 12458184 bytes\n",
            "Found file \"IISWP2_5_EcalasIesporte - Sistema Mobile para a Inclusão Social de Crianças entre 5 a 15 anos com Dificuldades Motoras, Através das Escalas EDM, HMS, MAE..gslides\" with length 182 bytes\n",
            "Found file \"Regras Apresentação.gdoc\" with length 182 bytes\n",
            "Found file \"descreva sobre O objetivo do plano de gerenciamen....gsheet\" with length 182 bytes\n",
            "Found file \"você pode me mostrar esses dados anteriores em um....gsheet\" with length 182 bytes\n",
            "Found file \"Cód.\n",
            "Aqui\n",
            " Item a ser adquirido\n",
            " Cód.\n",
            "EAP\n",
            " Motivo....gsheet\" with length 182 bytes\n",
            "Found file \"27HVE71CK8A.pdf\" with length 139881 bytes\n",
            "Found file \"pythonProject1.rar\" with length 11705362 bytes\n",
            "Found file \"EMPREENDEDORISMO_EP (1).gslides\" with length 182 bytes\n",
            "Found file \"Dados_Imersão .gsheet\" with length 182 bytes\n",
            "Found file \"Exercício 01 - Empreendedorismo e Inovação.gdoc\" with length 182 bytes\n",
            "Found file \"EMPREENDEDORISMO_EP.gslides\" with length 182 bytes\n",
            "Found file \"Empreendedorismo e Inovação.ppt\" with length 58531840 bytes\n",
            "Found file \"pythonProject1.zip\" with length 13372852 bytes\n",
            "Found file \"Fundamentos_da_matematica_elementar_colorido (1).rar\" with length 202491788 bytes\n",
            "Found file \"Fundamentos_da_matematica_elementar_colorido.rar\" with length 202491788 bytes\n",
            "Found file \"Certificados (2).pdf\" with length 2885414 bytes\n",
            "Found file \"Certificados (1).pdf\" with length 2885414 bytes\n",
            "Found file \"TP-1_Algoritmos de agrupamento.gdoc\" with length 182 bytes\n",
            "Found directory \"Google AI Studio\"\n",
            "Found file \"Marketing I.gslides\" with length 182 bytes\n",
            "Found file \"NATUSAUDE.gdoc\" with length 182 bytes\n",
            "Found file \"Declaração do RU.pdf\" with length 32019 bytes\n",
            "Found file \"Screenshot_20240912-144150~2 (1).png\" with length 78085 bytes\n",
            "Found file \"Screenshot_20240912-144150~2.png\" with length 78085 bytes\n",
            "Found file \"Comprovante 30 (1).pdf\" with length 648445 bytes\n",
            "Found file \"Comprovante 30.pdf\" with length 648445 bytes\n",
            "Found file \"report (39).pdf\" with length 31607 bytes\n",
            "Found directory \"ITS087-Paradigmas-LP\"\n",
            "Found directory \"dataset-resized\"\n",
            "Found file \"Curriculum vitae.pdf\" with length 127622 bytes\n",
            "Found file \"garbage_classifcat.rar\" with length 235632500 bytes\n",
            "Found directory \"Garbage classification\"\n",
            "Found file \"Apresentação-WP3-ICET-2-IoT_Bolsista_1-Bolsista_2-I-SWP3 (1).pptx\" with length 18556148 bytes\n",
            "Found file \"Apresentação-WP3-ICET-2-IoT_Bolsista_1-Bolsista_2-I-SWP3.pptx\" with length 7856694 bytes\n",
            "Found directory \"projeto.zip (Unzipped Files)\"\n",
            "Found file \"Tutorial da Aplicação.odt\" with length 28300 bytes\n",
            "Found file \"SD-aula02.gslides\" with length 182 bytes\n",
            "Found file \"SD-aula03.gdoc\" with length 182 bytes\n",
            "Found file \"Aula NTP.gslides\" with length 182 bytes\n",
            "Found file \"Cópia de Cópia de camResiduo.ipynb\" with length 686303 bytes\n",
            "Found file \"Documento sem título.gdoc\" with length 182 bytes\n",
            "Found file \"TRAB_BD - TRASPORTE CARGA AÉREA.docx\" with length 212594 bytes\n",
            "Found file \"HE_Sergio_Carmo.pdf\" with length 20778 bytes\n",
            "Found file \"garbage_classifcat-.rar\" with length 49188778 bytes\n",
            "Found directory \"Treinamento Smart Bins\"\n",
            "Found file \"Apresentação-WP3-ICET-2-IoT_Bolsista_1-Bolsista_2-II-SWP3.pptx\" with length 14547119 bytes\n",
            "Found directory \"garbage_classifcat\"\n",
            "Found file \"Apresentação K-Means.gslides\" with length 182 bytes\n",
            "Found file \"Lixeira Reconhecendo o residuo Plastico.mp4\" with length 5143986 bytes\n",
            "Found file \"Apresentação SUPER Week Techg-Ano 5-WP3-ICET-2-IoT_Bolsista_1-Bolsista_2.gslides\" with length 182 bytes\n",
            "Found file \"STW5_WP3-IoT_Sergio_Carmo.Felipe_Barbosa.gslides\" with length 182 bytes\n",
            "Found file \"Classificação do Residuo 1.mp4\" with length 8335425 bytes\n",
            "Found file \"Funcionamento da Lixeira.mp4\" with length 5143986 bytes\n",
            "Found file \"Lixeira Inteligente para Classificação de Resíduos Sólidos.gslides\" with length 182 bytes\n",
            "Found file \"ARTIGO_SMATR BINS.gdoc\" with length 182 bytes\n",
            "Found file \"H_E_Sergio_Ademir_Rocha (1).pdf\" with length 22066 bytes\n",
            "Found file \"Residência.pdf\" with length 362934 bytes\n",
            "Found file \"Curriculo Lattes.pdf\" with length 504881 bytes\n",
            "Found file \"CPF e Identidade.pdf\" with length 718538 bytes\n",
            "Found file \"Comprovante de Matricula.pdf\" with length 5054 bytes\n",
            "Found file \"Histórico Escolar.pdf\" with length 22072 bytes\n",
            "Found file \"Certificados.pdf\" with length 5451633 bytes\n",
            "Found file \"Questionário sobre o projeto inclusão social de crianças entre 5 a 15 anos com dificuldades motoras..gform\" with length 182 bytes\n",
            "Found file \"Cópia de Formulário sem título.gform\" with length 182 bytes\n",
            "Found file \"Modelo_de_Slides_para_apresentação_do_artigo[1].pptx\" with length 142548 bytes\n",
            "Found file \"Modelo_de_Slides_para_apresentação_do_artigo[1] (1).gslides\" with length 182 bytes\n",
            "Found file \"Modelo_de_Slides_para_apresentação_do_artigo[1].gslides\" with length 182 bytes\n",
            "Found file \"Olá Meu nome é Sérgio Ademir Rocha sou aluno do curso de Engenharia de Software estou fazendo uma pesquisa sobre o Protó~e onde  as notas 1 e 2 são negativas, 4 e 5 são positivas, e 3 é neutro.   Ficarei muito grato a todos que responderem ! .gform\" with length 182 bytes\n",
            "Found file \"IMG_20250401_102955.jpg\" with length 817109 bytes\n",
            "Found file \"IMG_20241004_140054.jpg\" with length 4311631 bytes\n",
            "Found file \"IMG_20240923_132250~2.jpg\" with length 151614 bytes\n",
            "Found file \"IMG_20231231_214919.jpg\" with length 2826550 bytes\n",
            "Found file \"IMG_20230906_131753.jpg\" with length 2412853 bytes\n",
            "Found file \"IMG_20230904_093541~3.jpg\" with length 651981 bytes\n",
            "Found file \"IMG_20221118_122930_HDR.jpg\" with length 3306067 bytes\n",
            "Found file \"IMG_20220102_141457.jpg\" with length 5063304 bytes\n",
            "Found file \"IMG_20211031_161304_565.jpg\" with length 1284008 bytes\n",
            "Found file \"H_E_Sergio_Carmo.pdf\" with length 28231 bytes\n",
            "Found file \"Certificados_2025.pdf\" with length 2294872 bytes\n",
            "Found file \"CÓDIGO LEGÍVEL.gslides\" with length 182 bytes\n",
            "Found file \"Artigo_NatuSaude_v1.gdoc\" with length 182 bytes\n",
            "Found file \"Cópia de Artigo_NatuSaude_v1.gdoc\" with length 182 bytes\n",
            "Found file \"H_E_Sergio_Ademir_Rocha.pdf\" with length 22219 bytes\n",
            "Found file \"Percepção dos Estudantes de Sistemas de Informação, Engenharia de Software e Engenharia de Produção sobre Ciência de Dados .gform\" with length 182 bytes\n",
            "Found file \"Percepção dos Estudantes de Sistemas de Informação, Engenharia de Software e Engenharia de Produção sobre Ciência de Dados  (respostas).gsheet\" with length 182 bytes\n",
            "Found file \"Relatório de Estratégias Adaptativas\n",
            "Este relatór....gsheet\" with length 182 bytes\n",
            "Found directory \"Estagio\"\n",
            "Found file \"RG.pdf\" with length 390790 bytes\n",
            "Found file \"RG (3).pdf\" with length 387323 bytes\n",
            "Found file \"Comprovante_Matricula.pdf\" with length 5174 bytes\n",
            "Found file \"Historico_SergioCarmo.pdf\" with length 23734 bytes\n",
            "Found file \"Curriculo Lates.pdf\" with length 895843 bytes\n",
            "Found file \"fatura-20439377-1760020528097.pdf\" with length 20933 bytes\n",
            "Found file \"meu_arquivo.csv\" with length 23 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9445b26a",
        "outputId": "534a080b-6c1d-4e17-b796-bc98f47be518"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify the folder path in Google Drive\n",
        "folder_path = '/content/drive/My Drive/Estagio/'\n",
        "\n",
        "# List files in the folder\n",
        "try:\n",
        "    for fn in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, fn)\n",
        "        if os.path.isfile(file_path):\n",
        "            file_size = os.path.getsize(file_path)\n",
        "            print('Found file \"{name}\" with length {length} bytes'.format(\n",
        "                name=fn, length=file_size))\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The folder '{folder_path}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para reconstruir e extrair embedding\n",
        "def reconstruct_face(image_path):\n",
        "    # Pré-processar imagem (resize para 224x224, normalize)\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "    img = (img / 255.0 - 0.5) * 2.0  # Normalização para [-1, 1]\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        coeff = sess.run('coeff:0', feed_dict={'input:0': img})  # Coeficientes (shape, exp, tex, etc.)\n",
        "        # Extrair embedding: Use coeff como feature vector para comparação\n",
        "        embedding = coeff[:, :257]  # Exemplo: identity + expression\n",
        "        return embedding"
      ],
      "metadata": {
        "id": "rHoKXipYdCAU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo de comparação (para reconhecimento)\n",
        "def compare_faces(emb1, emb2):\n",
        "    similarity = np.dot(emb1, emb2.T) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
        "    return similarity > 0.7  # Threshold ajustável"
      ],
      "metadata": {
        "id": "iGBGR-NjdMiK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo de comparação (para reconhecimento)\n",
        "def compare_faces(emb1, emb2):\n",
        "    similarity = np.dot(emb1, emb2.T) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
        "    return similarity > 0.7  # Threshold ajustável"
      ],
      "metadata": {
        "id": "Ox1VH1G3dVjg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#AQUI CÓDIGO PARA CAMERA 3D PLUGADA NO EQUIPAMENTO\n",
        "# Real-time com câmera 3D (ex.: RealSense para depth)\n",
        "#import pyrealsense2 as rs\n",
        "#import numpy as np\n",
        "#import cv2\n",
        "\n",
        "#pipeline = rs.pipeline()\n",
        "#config = rs.config()\n",
        "#config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
        "#config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
        "\n",
        "#try:\n",
        " #   pipeline.start(config)\n",
        "  #  while True:\n",
        "   #     frames = pipeline.wait_for_frames()\n",
        "    #    color_frame = frames.get_color_frame()\n",
        "     #   depth_frame = frames.get_depth_frame()\n",
        "      #  if not color_frame or not depth_frame:\n",
        "       #     continue\n",
        "        #color_img = np.asanyarray(color_frame.get_data())\n",
        "        # Salve temp e reconstrua (integre depth para 3D: combine RGB com depth map)\n",
        "        #cv2.imwrite('temp.jpg', color_img)\n",
        "        #emb = reconstruct_face('temp.jpg')\n",
        "        # Compare com database de embeddings de alunos\n",
        "        #print(\"Aluno reconhecido?\")\n",
        "        # Treinamento: Use script do repo para fine-tune em seu dataset (adapte train.py se disponível)\n",
        "#except RuntimeError as e:\n",
        " #   print(f\"Error starting camera pipeline: {e}\")\n",
        "  #  print(\"Please ensure your RealSense camera is connected.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X_lbrOhSnfzV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usando a camera do notebook\n",
        "# Captura uma imagem da câmera do notebook (via navegador)\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "def take_photo(filename='foto.jpg', quality=0.8):\n",
        "    js_code = '''\n",
        "        async function takePhoto(quality) {\n",
        "          const div = document.createElement('div');\n",
        "          const capture = document.createElement('button');\n",
        "          capture.textContent = ' Capturar';\n",
        "          div.appendChild(capture);\n",
        "          document.body.appendChild(div);\n",
        "          const video = document.createElement('video');\n",
        "          video.style.display = 'block';\n",
        "          const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "          document.body.appendChild(video);\n",
        "          video.srcObject = stream;\n",
        "          await video.play();\n",
        "          google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "          await new Promise((resolve) => capture.onclick = resolve);\n",
        "          const canvas = document.createElement('canvas');\n",
        "          canvas.width = video.videoWidth;\n",
        "          canvas.height = video.videoHeight;\n",
        "          const context = canvas.getContext('2d');\n",
        "          context.drawImage(video, 0, 0, canvas.width, canvas.height);\n",
        "          stream.getTracks().forEach(track => track.stop());\n",
        "          div.remove();\n",
        "          video.remove();\n",
        "          return canvas.toDataURL('image/jpeg', quality);\n",
        "        }\n",
        "\n",
        "        takePhoto(''' + str(quality) + ''');\n",
        "    '''\n",
        "    data = eval_js(js_code) # Evaluate the Javascript code string\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    return filename\n",
        "\n",
        "# Captura e mostra a imagem\n",
        "filename = take_photo()\n",
        "print(\"Imagem capturada:\", filename)\n",
        "Image.open(filename)"
      ],
      "metadata": {
        "id": "MEfzMsjfdcP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* TESTES\n",
        "- Adicionais de funcionamentos  \n",
        "- Outros ajustes"
      ],
      "metadata": {
        "id": "X-d8AdcQrHSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Só uma breve verificação da versão\n",
        "# Célula 1 - execute no Colab\n",
        "# Instalações\n",
        "!pip install mtcnn==0.1.1 tensorflow==2.14.1 opencv-python-headless==4.7.0.72 keras-preprocessing --quiet\n",
        "\n",
        "# Monte o Google Drive (se o dataset está no Drive)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "YsPj5eBEdrxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.version)"
      ],
      "metadata": {
        "id": "C3WjBDShXXg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.18.0"
      ],
      "metadata": {
        "id": "sh8r3qPlXf_s",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.18.0"
      ],
      "metadata": {
        "collapsed": true,
        "id": "15yoQDodY3X-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "CGRB-brPZABC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QkN2ErSvXgq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Create a sample DataFrame (replace this with your actual data)\n",
        "df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n",
        "# Exemplo: salvar um DataFrame\n",
        "df.to_csv('/content/drive/MyDrive/meu_arquivo.csv')"
      ],
      "metadata": {
        "id": "Uk4Rps3cXYhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Célula 2 - ajustar o caminho do dataset abaixo\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "DATASET_ROOT = Path('/content/drive/My Drive/Estagio/')  # ajuste aqui\n",
        "\n",
        "# checar estrutura\n",
        "if not DATASET_ROOT.exists():\n",
        "    raise FileNotFoundError(f\"Dataset não encontrado em {DATASET_ROOT}. Ajuste o caminho.\")\n",
        "\n",
        "classes = [p.name for p in DATASET_ROOT.iterdir() if p.is_dir()]\n",
        "print(f\"Encontradas {len(classes)} classes (identidades). Exemplos: {classes[:10]}\")\n"
      ],
      "metadata": {
        "id": "4gaDjeQjdjdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mtcnn import MTCNN\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "detector = MTCNN()\n",
        "\n",
        "# Aqui está o ajuste principal:\n",
        "PROCESSED_ROOT = Path('/content/processed_rosto_alunos')\n",
        "PROCESSED_ROOT.mkdir(exist_ok=True)  # cria a pasta se não existir\n",
        "\n",
        "for cls in classes:\n",
        "    src_dir = DATASET_ROOT / cls\n",
        "    out_dir = PROCESSED_ROOT / cls  # Save processed images to a subdirectory named after the class\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    for img_file in tqdm(list(src_dir.glob('*')), desc=f\"Processando {cls}\"):\n",
        "        try:\n",
        "            img = cv2.imread(str(img_file))\n",
        "            if img is None:\n",
        "                continue\n",
        "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            results = detector.detect_faces(img_rgb)\n",
        "            if results:\n",
        "                best = max(results, key=lambda x: x['box'][2]*x['box'][3])\n",
        "                x, y, w, h = best['box']\n",
        "                x, y = max(0, x), max(0, y)\n",
        "                crop = img_rgb[y:y+h, x:x+w]\n",
        "                face = cv2.resize(crop, (160,160))\n",
        "                Image.fromarray(face).save(out_dir / img_file.name)\n",
        "        except Exception as e:\n",
        "            pass\n",
        "\n",
        "print(\" Extração de faces concluída! Faces salvas em:\", PROCESSED_ROOT)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3cFPtBLV_3WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04a37af5",
        "collapsed": true
      },
      "source": [
        "!pip install --upgrade --no-deps --force-reinstall mtcnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a278f90e"
      },
      "source": [
        "!pip install lz4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "PROCESSED_ROOT = \"/content/processed_rosto_alunos\"\n",
        "\n",
        "# Verifica se o diretório existe\n",
        "print(\"Existe:\", os.path.exists(PROCESSED_ROOT))\n",
        "\n",
        "# Lista os arquivos encontrados\n",
        "for root, dirs, files in os.walk(PROCESSED_ROOT):\n",
        "    print(f\"\\n Pasta: {root}\")\n",
        "    for f in files:\n",
        "        print(\"  -\", f)\n"
      ],
      "metadata": {
        "id": "ENKjjvo0crD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "# Caminhos\n",
        "src_dir = \"/content/processed_rosto_alunos/rostos_CSV\"\n",
        "dst_dir = \"/content/processed_rosto_alunos/treinamento/rostos_alunos\"\n",
        "target_dir = os.path.join(dst_dir, \"classe_unica\")\n",
        "\n",
        "# Check if the source directory is empty and provide a helpful message\n",
        "if not os.listdir(src_dir):\n",
        "    print(f\"Error: Source directory is empty: {src_dir}\")\n",
        "    print(\"Please run the face extraction step (cell 3cFPtBLV_3WW) first and ensure it successfully saves images to this directory.\")\n",
        "else:\n",
        "    # List contents of src_dir to help identify the correct image folder\n",
        "    print(f\"Contents of {src_dir}: {os.listdir(src_dir)}\")\n",
        "\n",
        "    # Cria a structure esperada: uma subpasta dentro de outra\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "    # Mover ou copiar as imagens\n",
        "    copied_count = 0\n",
        "    for img in os.listdir(src_dir):\n",
        "        if img.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "            shutil.copy(os.path.join(src_dir, img), os.path.join(target_dir, img))\n",
        "            copied_count += 1\n",
        "\n",
        "    print(f\"Imagens reorganizadas com sucesso! Total copiado: {copied_count}\")\n",
        "\n",
        "    # Add a small delay\n",
        "    time.sleep(1)\n",
        "\n",
        "    # Check if images are in the target directory AFTER the delay\n",
        "    target_dir_contents = os.listdir(target_dir)\n",
        "    print(f\"Conteúdo de {target_dir}: {target_dir_contents}\")\n",
        "\n",
        "    # Create the dataset manually from a list of file paths\n",
        "    image_files = [os.path.join(target_dir, f) for f in os.listdir(target_dir) if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
        "\n",
        "    if not image_files:\n",
        "        print(f\"Error: No image files found in the target directory: {target_dir}\")\n",
        "        print(\"Please check if the images were copied correctly.\")\n",
        "    else:\n",
        "        list_ds = tf.data.Dataset.from_tensor_slices(image_files)\n",
        "        list_ds = list_ds.shuffle(len(image_files), seed=42)\n",
        "\n",
        "        def process_path(file_path):\n",
        "            img = tf.io.read_file(file_path)\n",
        "            # Try decoding as both JPEG and PNG\n",
        "            try:\n",
        "                img = tf.image.decode_jpeg(img, channels=3)\n",
        "            except tf.errors.InvalidArgumentError:\n",
        "                img = tf.image.decode_png(img, channels=3)\n",
        "\n",
        "            img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "            img = tf.image.resize(img, [224, 224])\n",
        "            # Since we have only one class, the label is always 0\n",
        "            label = 0\n",
        "            return img, label\n",
        "\n",
        "        train_ds = list_ds.map(process_path)\n",
        "\n",
        "        # Set up the dataset for training\n",
        "        BATCH_SIZE = 32\n",
        "        train_ds = train_ds.batch(BATCH_SIZE)\n",
        "        train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "        print(\" Dataset created successfully using tf.data.Dataset.from_tensor_slices!\")"
      ],
      "metadata": {
        "id": "fs0bAr8Pe5-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "# Procura todas as imagens dentro das subpastas\n",
        "imagens = glob.glob(\"/content/processed_rosto_alunos/**/*.*\", recursive=True)\n",
        "imagens_png = [img for img in imagens if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "print(f\"Total de imagens encontradas: {len(imagens_png)}\")\n",
        "print(\"\\nAlguns exemplos:\")\n",
        "for img in imagens_png[:10]:\n",
        "    print(img)\n"
      ],
      "metadata": {
        "id": "FdhpxBHxcu2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PLZvjGmIdCQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  criando o treinamento\n",
        "\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Point to the directory containing the images for the single class\n",
        "IMAGE_DIR = Path('/content/processed_rosto_alunos/treinamento/rostos_alunos/classe_unica')\n",
        "IMAGE_SIZE = (160,160)\n",
        "BATCH_SIZE = 32\n",
        "SEED = 42\n",
        "\n",
        "# Create a list of image file paths manually\n",
        "image_files = [str(f) for f in IMAGE_DIR.glob('*') if f.suffix.lower() in ['.png', '.jpg', '.jpeg']]\n",
        "\n",
        "if not image_files:\n",
        "    print(f\"Error: No image files found in the directory: {IMAGE_DIR}\")\n",
        "else:\n",
        "    list_ds = tf.data.Dataset.from_tensor_slices(image_files)\n",
        "    list_ds = list_ds.shuffle(len(image_files), seed=SEED)\n",
        "\n",
        "    def process_path(file_path):\n",
        "        img = tf.io.read_file(file_path)\n",
        "        # Try decoding as both JPEG and PNG\n",
        "        try:\n",
        "            img = tf.image.decode_jpeg(img, channels=3)\n",
        "        except tf.errors.InvalidArgumentError:\n",
        "            img = tf.image.decode_png(img, channels=3)\n",
        "\n",
        "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "        img = tf.image.resize(img, IMAGE_SIZE)\n",
        "        # Since we have only one class, the label is always 0\n",
        "        label = 0\n",
        "        return img, label\n",
        "\n",
        "    train_ds = list_ds.map(process_path)\n",
        "\n",
        "    # Set up the dataset for training\n",
        "    train_ds = train_ds.batch(BATCH_SIZE)\n",
        "    train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    # Manually set class name and number of classes\n",
        "    class_names = [\"classe_unica\"]\n",
        "    num_classes = len(class_names)\n",
        "\n",
        "    print(f\" Dataset pronto com {tf.data.experimental.cardinality(train_ds).numpy() * BATCH_SIZE} imagens na classe:\", class_names)\n",
        "\n",
        "    # val_ds is not created with this approach, set to None\n",
        "    val_ds = None"
      ],
      "metadata": {
        "id": "TZALb_SZEzJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Ensure that the dataset creation cell (TZALb_SZEzJe) has been run\n",
        "# as it defines the dataset structure and IMAGE_SIZE.\n",
        "\n",
        "# Explicitly set num_classes based on the dataset created in the previous cell (TZALb_SZEzJe)\n",
        "# Assuming a single class (\"classe_unica\") is being used.\n",
        "num_classes = 1\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.05),\n",
        "    layers.RandomZoom(0.08),\n",
        "])\n",
        "\n",
        "base_model = tf.keras.applications.MobileNetV2(\n",
        "    input_shape=IMAGE_SIZE + (3,),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "base_model.trainable = False  # congela camadas base\n",
        "\n",
        "model = models.Sequential([\n",
        "    data_augmentation,\n",
        "    # Removed tf.keras.applications.mobilenet_v2.preprocess_input\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "HtqV_-2pE-kX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Ensure that the dataset creation cell (TZALb_SZEzJe) has been run\n",
        "# as it defines the 'train_ds' and 'val_ds' variables.\n",
        "\n",
        "# Explicitly check if train_ds is defined\n",
        "try:\n",
        "    train_ds is not None\n",
        "except NameError:\n",
        "    print(\"Error: train_ds is not defined.\")\n",
        "    print(\"Please run the dataset creation cell (TZALb_SZEzJe) first.\")\n",
        "    raise # Re-raise the error to stop execution\n",
        "\n",
        "CKPT_PATH = '/content/face_classifier_mobilenetv2.h5'\n",
        "\n",
        "callbacks = [\n",
        "    ModelCheckpoint(CKPT_PATH, monitor='val_accuracy', save_best_only=True, verbose=1),\n",
        "    EarlyStopping(monitor='val_accuracy', patience=6, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
        "]\n",
        "\n",
        "EPOCHS = 15\n",
        "\n",
        "# Only include validation_data if val_ds is not None\n",
        "if 'val_ds' in locals() and val_ds is not None:\n",
        "    history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=callbacks)\n",
        "else:\n",
        "    # If val_ds is not defined or is None, train without validation data\n",
        "    print(\"Note: val_ds is not available, training without validation data.\")\n",
        "    history = model.fit(train_ds, epochs=EPOCHS, callbacks=callbacks)"
      ],
      "metadata": {
        "id": "SQ9b2rTDHYWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.trainable = True\n",
        "fine_tune_at = 100  # libera apenas as últimas camadas\n",
        "\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "EPOCHS_FINE = 10\n",
        "history_fine = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS_FINE, callbacks=callbacks)\n"
      ],
      "metadata": {
        "id": "j1QzKycGHlJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content -type f \\( -iname \"*.jpg\" -o -iname \"*.png\" \\) | head -n 20\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_oSMp7L5IHHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mtcnn import MTCNN\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img_path = str(list(DATASET_ROOT.rglob(\"*.jpg\"))[0])  # pega a primeira imagem original\n",
        "print(\"Exemplo de imagem:\", img_path)\n",
        "\n",
        "img = cv2.imread(img_path)\n",
        "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "detector = MTCNN()\n",
        "faces = detector.detect_faces(img_rgb)\n",
        "\n",
        "print(f\"Detectadas {len(faces)} faces\")\n",
        "if faces:\n",
        "    x, y, w, h = faces[0]['box']\n",
        "    cv2.rectangle(img_rgb, (x,y), (x+w,y+h), (0,255,0), 2)\n",
        "    plt.imshow(img_rgb)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Cfxzci_iIS1U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}